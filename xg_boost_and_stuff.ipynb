{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get into XGBoost :)\n",
    "\n",
    "# XGBoost is the leading model for working with data\n",
    "# XGBoost models focus heavily on model tuning for peak accuracy\n",
    "    # XGBoost is an implementation of the Gradient Boosted Decicion Tree algorithm\n",
    "    # What are Gradient Boosted Deicion Trees?\n",
    "        # A model that goes through cycles that repeatedly builds new models and combines them into an ensemble model\n",
    "        \n",
    "            # Cycles: Naive Model ====> Calculate Errors ======> Build Model Predicting Errors ======> Add Last Model Ensemble ======> Calculate Errors\n",
    "            \n",
    "        # What we're doing here is:\n",
    "            # 1. we're calculating the errors for each observation in the dataset\n",
    "            # 2. We then build a new model predicting those errors\n",
    "            # 3. We add those predictors to the \"ensemble\" of models then repeat\n",
    "\n",
    "    # To make a prediction, we add the predictions from each previous model. We use these to continuously fine tune our model to calculate new errors with new data to build the next one\n",
    "    # XGBoost requires One key factor though:\n",
    "        # We need base predictions to start the cycle!!\n",
    "        # Initial predictions can be naive, and wildly inaccurate, this will kickstart the process and get fine tuned throughout it\n",
    "        \n",
    "# Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthewquinn/.local/share/virtualenvs/data-science-learning-e1DZx5Tr/lib/python3.7/site-packages/ipykernel_launcher.py:20: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "# Initial Setup\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# Import our dataset and drop missing values from 'Withdrew' column\n",
    "df = pd.read_csv('./datasets/withdrawl_data.cvs')\n",
    "df.dropna(axis=0, subset=['Withdrew'], inplace=True)\n",
    "\n",
    "# Update 'No/Yes' object to '0/1' int64\n",
    "df.update(df.Withdrew.replace('Yes',1))\n",
    "df.update(df.Withdrew.replace('No',0))\n",
    "\n",
    "# Define x and y of\n",
    "y = df.Withdrew\n",
    "x = df.drop(['Withdrew'], axis=1).select_dtypes(exclude=['object'])\n",
    "\n",
    "# Split our datasets \n",
    "model_x, test_x, model_y, test_y = train_test_split(x.as_matrix(), y.as_matrix(), test_size=0.25)\n",
    "\n",
    "# Impute the data\n",
    "    # .fit_transform fits the data to the model, then transforms it\n",
    "    # .transorm just imputes all missing values in the data\n",
    "my_imputer = Imputer()\n",
    "model_x = my_imputer.fit_transform(model_x)\n",
    "test_x = my_imputer.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now lets build and fit the model with XGBoost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Define Model\n",
    "xgb_model = XGBRegressor()\n",
    "\n",
    "# Fit the model\n",
    "xgb_model.fit(model_x, model_y, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03286496,  0.11411265,  0.144741  ,  0.47576138,  0.09272575,\n",
       "        0.05374989,  0.4620709 ,  0.0028688 ,  0.46653074,  0.9864986 ,\n",
       "        0.98769724,  0.63574755, -0.07120657,  0.9812927 ,  0.97256005,\n",
       "        0.9879961 ,  0.05758655,  0.60597897,  1.0065932 ,  1.0424104 ,\n",
       "        0.70486283,  0.05088085,  0.5135442 ], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's make some predictions with our model\n",
    "predictions = xgb_model.predict(test_x)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.3474625854388527\n"
     ]
    }
   ],
   "source": [
    "# Now let's evaluate our model using MAE\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(\"MAE: \" + str(mean_absolute_error(predictions, test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL TUNING: Now let's fine tune our model to increase its accuracy and training speed\n",
    "\n",
    "# Parameters to know\n",
    "    # 1. n_estimators\n",
    "        # - specifies the # of times to go through the model cycle\n",
    "        # - n_estimators help find the sweet spot between underfitting and overfitting\n",
    "            # underfitting: inaccurate predictions on model data and new data\n",
    "            # overfitting: highly accurate predictions on model data, inaccurate on new data\n",
    "            # - n_estimators=1000\n",
    "            \n",
    "    # 2. early_stopping_rounds\n",
    "        # - is a way to automatically find the ideal n_estimators\n",
    "        # - This causes a model to automatically stop iterating when validation scores stop improving\n",
    "        # - it's smart enough to stop for n_estimators and smart enough to find the optimal time\n",
    "        # - specify the number of rounds to iterate through a cycle\n",
    "            # - early_stopping_rounds=5\n",
    "                # stop after 5 rounds of iterating\n",
    "                \n",
    "    # 3. learning_rate\n",
    "        # - Instead of adding the predictions from each component, multiple them by a small number before adding them\n",
    "        # - This reduces the model's possibility of overfitting\n",
    "        # - You can set a high level of n_estimators without overfitting\n",
    "        # - A small learning rate will yield more XGBoost models\n",
    "\n",
    "    # 4. n_jobs\n",
    "        # - n_jobs will help make building models parallel so models build faster\n",
    "        # - on large datasets, small datasets won't really be impacted much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.05, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=1000,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's re-create our fit by adding n_estimates, and early_stopping rounds\n",
    "\n",
    "# Now lets build and fit the model with XGBoost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Define Model with n_estimators and s learning_rate\n",
    "new_xgb_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=1)\n",
    "\n",
    "# Fit the model with early_stopping_rounds\n",
    "# eval_set allows you to evaluate your test data against this fit\n",
    "xgb_model.fit(model_x, model_y, early_stopping_rounds=5, eval_set=[(test_x,test_y)],verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.3379264888556107\n"
     ]
    }
   ],
   "source": [
    "# Now let's make some predictions with our model\n",
    "# Fit the model\n",
    "new_xgb_model.fit(model_x, model_y, verbose=False)\n",
    "new_predictions = new_xgb_model.predict(test_x)\n",
    "print(\"MAE: \" + str(mean_absolute_error(new_predictions, test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
